{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data\n",
    "- Check the date format\n",
    "- Use the describe funtion for both (numeric stats and object stats) to check if there are any missing values in the    dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('31009751_dirty_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer review analysis\n",
    "- By using the sentimental intensity analyzer we first calculate the compound scores:\n",
    "    if the compound score is greater than or equal to 0.05, then consider the review to be positive\n",
    "    if the compound score is less than 0.05, then consider the review to be negative\n",
    "- Found a 'None' component in the reviews column; replaced sentiment of 'None' with True since sentiment analyzer              categorises 'None' to be negative (False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = SentimentIntensityAnalyzer()\n",
    "df['is_happy_customer'] = df['latest_customer_review'].apply(\n",
    "    lambda x: senti.polarity_scores(x)['compound'] >= 0.05).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_happy_customer'].replace(1,True, inplace=True)\n",
    "df['is_happy_customer'].replace(0,False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing 'None' review's sentiment as True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[31,'is_happy_customer'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[30:32,] # Checking the updated column in  a dataframe format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date correction\n",
    "- Split the date on '-' so that we get each year, month and day separately. \n",
    "- Make sure the first element is the year (2019) and not month or day. \n",
    "    if a month or a day is found, swap it with the year\n",
    "    if the second element of the splitted date is greater than 12, then swap it with the last element \n",
    "- Join the splitted date on '-' and update the df with this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows(): # Looping through each date in the dataframe\n",
    "    split_date = row['date'].split('-') # Splitting the date string based on '-'\n",
    "    \n",
    "    # Check if the first element is not the year\n",
    "    if split_date[0] != '2019':  \n",
    "        # Swapping the first and third element of the splitted date\n",
    "        split_date[0], split_date[2] = split_date[2], split_date[0] \n",
    "       \n",
    "        # Updating the dataframe with the corrected date\n",
    "        df['date'][index] = '-'.join(split_date) \n",
    "    # Check if the second element is greater than 12 \n",
    "    # (to determine if its the month or day) \n",
    "    if int(split_date[1]) > 12: \n",
    "        # Swapping the second and the third elements \n",
    "        # incase the aboe condition is true\n",
    "        temp = split_date[1] \n",
    "        split_date[1] = split_date[2]\n",
    "        split_date[2] = temp\n",
    "        \n",
    "        # Updating the dataframe with the corrected date\n",
    "        df['date'][index] = '-'.join(split_date) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest_warehouse and distance_to_nearest_warehouse\n",
    "### Check if there are only 3 values for the warehouse column (which should be the ideal case)\n",
    "- We find there are 6 unique values in this column\n",
    "- But the error here is that the names of the ware houses are the same but there is a mismatch in their case \n",
    "- Replacing the smaller case names with the camel case names (E.g. 'bakers' is replaced with 'Bakers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unique values in nearest_warehouse column\n",
    "np.unique(df['nearest_warehouse'].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'bakers' with 'Bakers'\n",
    "df.replace('bakers','Bakers', inplace = True) \n",
    "\n",
    "# Replace 'thompson' with 'Thompson'\n",
    "df.replace('thompson','Thompson', inplace = True) \n",
    "\n",
    "# Replace 'nickolson' with 'Nickolson'\n",
    "df.replace('nickolson','Nickolson', inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance_to_nearest_warehouse\n",
    "- Defining a function to calculate the haversine distance between two coordinates\n",
    "- Reading in the warehouse's csv to get the details of each warehouse\n",
    "- Calculate distance of each customer location with each of the three warehouses\n",
    "- Then calculate which is the minimum distance and its indices\n",
    "- Loop through the warehouse_csv file and use the above mentioned indices to fetch the name of the warehouse\n",
    "- Update the dataframe with the new values found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import * # import everything from math package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the distance between customer loc to the nearest warehouse\n",
    "\n",
    "def haversine_fun(latitude1, longitude1, latitude2, longitude2):\n",
    "    # Given that the radius of the earth should be taken 6378 km \n",
    "    earth_radius = 6378 \n",
    "    \n",
    "    # Take the difference of the two latitudes and convert them to radians\n",
    "    lat_diff = radians(latitude2 - latitude1) \n",
    "    \n",
    "    # Take the difference of the two Longitudes and convert them to radians\n",
    "    lon_diff = radians(longitude2 - longitude1) \n",
    "    \n",
    "    # Convert the first latitude to radians\n",
    "    latitude1 = radians(latitude1) \n",
    "    \n",
    "    # Convert the second latitude to radians\n",
    "    latitude2 = radians(latitude2) \n",
    "\n",
    "    x = sin(lat_diff/2)**2 + cos(latitude1)*cos(latitude2)*sin(lon_diff/2)**2 \n",
    "    y = 2*asin(sqrt(x))\n",
    "\n",
    "    return earth_radius * y # Returns the distance between two coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the warehouse csv to get the coordinates \n",
    "# and names of each earehouses\n",
    "df1 = pd.read_csv('warehouses.csv') \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the distances from each warehouse to the customer loc\n",
    "\n",
    "# Contains all the 3 distances from each warehouse for each customer\n",
    "main_list = []   \n",
    "for index,rows in df.iterrows():\n",
    "    \n",
    "    # All the 3 distances of a customer from each earehouse\n",
    "    dist_list = [] \n",
    "    for index1, rows1 in df1.iterrows():\n",
    "        # Using haversince function to calculate the \n",
    "        # distance between the customer and each warehouse\n",
    "        dist_list.append(haversine_fun(\n",
    "            rows['customer_lat'],rows['customer_long'], \n",
    "            rows1['lat'], rows1['lon']))\n",
    "    main_list.append(dist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the index and minimum distance from the above list for each customer\n",
    "# List contains the index of the minimum distance for all the rows\n",
    "new = [] \n",
    "\n",
    "# List contains the minimum distance for all the rows\n",
    "min_dist = [] \n",
    "for i in main_list:\n",
    "    new.append(i.index(min(i)))\n",
    "    \n",
    "    # Rounding off the distance to 4 decimals\n",
    "    min_dist.append(round(min(i),4)) \n",
    "column_name = np.array(new) \n",
    "\n",
    "# Creating a new column which will contain the corresponding indices of warehouse names\n",
    "df['indexo'] = column_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the dirty values in the distance to nearest warehouse \n",
    "# column by replacing it entirely with the new calculated values\n",
    "min_dist = np.array(min_dist)\n",
    "df['distance_to_nearest_warehouse'] = min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the warehouse names based on the indices of their corresponding minimum dist\n",
    "df['nearest_warehouse'] = df['indexo'].apply(\n",
    "    lambda x: df1.loc[0,'names'] if x==0 else \n",
    "    (df1.loc[1,'names'] if x==1 else df1.loc[2,'names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distance_to_nearest_warehouse'] # To print and check the distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lat Long correction\n",
    "> By default the values of a Latitude ranges from -90 to 90 and that of Longitude ranges from -180 to 180. But few of the rows have interchanged values for their Lat and Long. Swapping those values should be done in order to correct the errors in these two columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swapping the customer_lat and customer_lon  \n",
    "for index, row in df.iterrows():\n",
    "    if abs(row['customer_lat']) > 100.0:\n",
    "        temp = row['customer_lat']\n",
    "        df.iloc[index, 7] = row['customer_long']\n",
    "        df.iloc[index, 8] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasons\n",
    "> Seasons have unmatched case. E.g \"Summer\" and \"summer\" etc. Also, for few of the rows the dates and the seasons do not match.\n",
    "\n",
    "- Splitting the date string to get each component individually (year-month-day)\n",
    "- Based on the month number in the date string, comparing the seasons and months as shown in link: http://www.bom.gov.au/climate/glossary/seasons.shtml\n",
    "- Replacing the appropriate season names with Camel case formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are only 4 seasons in the supplied csv file\n",
    "np.unique(df['season'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the season names with the month number \n",
    "# found after splitting the date string \n",
    "for index, row in df.iterrows():\n",
    "    if row['date'].split('-')[1] in ['09','10','11']:\n",
    "        df.iloc[index, 11] = 'Spring'\n",
    "    if row['date'].split('-')[1] in ['12','01','02']:\n",
    "        df.iloc[index, 11] = 'Summer'\n",
    "    if row['date'].split('-')[1] in ['03','04','05']:\n",
    "        df.iloc[index, 11] = 'Autumn'\n",
    "    if row['date'].split('-')[1] in ['06','07','08']:\n",
    "        df.iloc[index, 11] = 'Winter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order price and order total\n",
    "> In this method, the item name in shopping_cart will act like 'variables' and no. of items will act as the 'coefficients'. And the entire list of items shopped by a customer will act like an equation which will be used for linear algebra computation \n",
    "\n",
    "1. Creating a new column (df1['shopping_cart_dict']) containing the shopping cart values but in dictionary format\n",
    "\n",
    "2. Sorting the column df1['shopping_cart_dict'] inorder to get similar shopping_cart values together\n",
    "\n",
    "3. Unzipping the shopping cart tuple and creating two new columns:-\n",
    "    - crazy_df['coeff'] --> which will have only the coefficients of the equation(shopping cart)\n",
    "    - crazy_df['variables'] --> which will have only the variables(items) of the equation(shopping cart)\n",
    "4. Create a new dataframe which will have all the items of a shopping cart and their respective number of occurances \n",
    "5. Merge this dataframe with crazy_df to get the number of equations present and have it in a column called 'value'\n",
    "6. Create another column named ('variable_count') to hold the number of variables in an equation\n",
    "7. Drop any row which has a null value in the column 'delivery_charges' (since it cannot be used to calculate the variable value using linalg function)\n",
    "8. Filter out this dataframe into chunks of dataframes which are square in nature (in other words keep only those dataframes whose number of equations and number of variables are the same!) and append it to a list.\n",
    "    - This list will have multiple square dataframes as each element \n",
    "9. Calculate the price of each product using linalg from numpy and store it in a dictionary:\n",
    "    - This dictionary will have prices for each product (no. of products is 10)\n",
    "10. Using this dictionary and the items in the shopping cart, the order price can be calculated. \n",
    "11. Using the calculated order_price, the order_total can be calculated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting shopping cart values to dictionary and sorting it\n",
    "df['shopping_cart_dict'] = df['shopping_cart'].apply(\n",
    "    lambda x: dict(eval(x))) \n",
    "\n",
    "# Sorting the items based on the keys of the dictionary\n",
    "df['shopping_cart_dict'] = df['shopping_cart_dict'].apply(\n",
    "    lambda x: list(sorted(x.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dataframe based on the new column just created\n",
    "crazy_df = df.sort_values('shopping_cart_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the coeff, variable names \n",
    "\n",
    "# Using the unzip functionality to separate out the items \n",
    "# and their corresponding item numbers in two \n",
    "# different columns (coefficient and variables)\n",
    "crazy_df['coeff'] = crazy_df['shopping_cart_dict'].apply(\n",
    "    lambda x: list(list(zip(*x))[1]))\n",
    "crazy_df['variables'] = crazy_df['shopping_cart_dict'].apply(\n",
    "    lambda x: list(zip(*x))[0])\n",
    "\n",
    "# Create a new dataframe which will hold the count of the variables\n",
    "variable_count_df = pd.DataFrame(columns=['items']) \n",
    "\n",
    "# 'items' column will have item names and 'value' will \n",
    "# have how many times the 'items' have repeated\n",
    "variable_count_df['items'] = crazy_df['variables'].value_counts().index\n",
    "variable_count_df['value'] = crazy_df['variables'].value_counts().values\n",
    "variable_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining both the data frames\n",
    "value_count_df = pd.merge(crazy_df, variable_count_df, \n",
    "                          left_on='variables', right_on='items', how='inner')\n",
    "\n",
    "\n",
    "# Calculating the variable count and retaining only square matrix\n",
    "# Create a new column to hold the number of variables in each equation \n",
    "value_count_df['variable_count'] = value_count_df['variables'].apply(\n",
    "    lambda x: len(x))\n",
    "\n",
    "# dataframes which has square matrix charateristics\n",
    "square_df = value_count_df[value_count_df['value'] == \n",
    "                           value_count_df['variable_count']]  \n",
    "\n",
    "# List of dataframes which has square matrix charateristics\n",
    "list_of_df = []\n",
    "for i in np.unique(square_df['variables'].values):\n",
    "    list_of_df.append(square_df[square_df['variables'] == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating price of each product using linalg from numpy\n",
    "# Empty dictionary to hold the product names and their respective price\n",
    "product_list = {} \n",
    "\n",
    "# Looping through the list of dataframes\n",
    "for i in list_of_df:  \n",
    "    \n",
    "    # Check if number of variables of an equation is equal to \n",
    "    # the number of equation (square matrix) in order \n",
    "    # to be used in simultaneous equation\n",
    "    if i['variable_count'].value_counts().index.values == \\\n",
    "    i['variable_count'].value_counts().values:\n",
    "        # Try except block is used to avoid the \"Singular matrix\" \n",
    "        # error thrown by linalg.solve() function which occurs \n",
    "        # when there are many solutions\n",
    "        \n",
    "        try:  \n",
    "            # Take the list of coefficients\n",
    "            a = np.array(list(i['coeff'].values))  \n",
    "            \n",
    "            # Take the list of order price\n",
    "            b = np.array(list(i['order_price'].values))  \n",
    "            x = np.linalg.solve(a, b) # Solve for all the \n",
    "            # variables in the equation\n",
    "            \n",
    "            # If the solution is correct and non-negative condition \n",
    "            if np.allclose(np.dot(a, x), b) and x[1] > 0:  \n",
    "                # is checked, then update the product_list\n",
    "                product_list.update(dict(zip(i['items'].values[0],x)))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the order price column in the original dataframe\n",
    "# List of order price at the end of running this code cell\n",
    "cal_order_price = []  \n",
    "\n",
    "# Looping through each shopping_cart value\n",
    "for i in df['shopping_cart'].values: \n",
    "    sum_val = 0 \n",
    "    \n",
    "    # Looping withing the shopping_cart items\n",
    "    for j in eval(i): \n",
    "        \n",
    "        # price of item is taken from the product_list \n",
    "        # dictionary and multiplied by the no. of items\n",
    "        sum_val += product_list[j[0]] * j[1] \n",
    "                                             \n",
    "        \n",
    "    cal_order_price.append(sum_val)  \n",
    "    \n",
    "# Updating the entire column of 'order_price'\n",
    "df['order_price'] = cal_order_price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the missing values in order total column \n",
    "# in original dataframe\n",
    "df['order_total'] = df['order_price'] * \\\n",
    "(1 - (df['coupon_discount']/100)) + df['delivery_charges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expedited delivery "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Expedited delivery is assumed to be correct since there is no easy way of checking if the values given are correct. One of going about doing this would be as follow:\n",
    "\n",
    "1. From missing_data file, filter out all the non null values.\n",
    "2. On this dataframe, for each season train a model to predict the is_expedited_delivery (True/False) keeping the delivery_charges column independent. \n",
    "3. Use this model to take in input form dirty_data's delivery_charges column and predict the is_expedited delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the first 15 columns from the dataframe\n",
    "df = df.iloc[:,0:16]\n",
    "df.to_csv('31009751_dirty_data_output.csv', index= False, date_format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('31009751_missing_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating missing values in nearest warehouse column\n",
    "#### Steps\n",
    "1. From the original dataframe filter out the rows for which nearest warehouse is NaN and save it in a new dataframe (empty_warehouse) \n",
    "2. For each of the customer calculate the distance between all the three warehouses using the haversine function and take it in a list\n",
    "3. From the above list select the minimum distance among the 3 distances and noting their respective indices\n",
    "4. Looping through the warehouses.csv file using the above indices to fetch the names of the warehouses and updating it in the missing column\n",
    "\n",
    "5. Updating the original dataframe using the sub-dataframe (empty_warehouse) mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering null values of warehouse from original \n",
    "# dataframe to a separate dataframe \n",
    "empty_warehouse = df1[df1['nearest_warehouse'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the warehouses csv\n",
    "df_warehouse = pd.read_csv('warehouses.csv')\n",
    "df_warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the distances from each warehouse to the customer loc\n",
    "main_list = [] \n",
    "# Looping through the dataframe which has NULL values in \n",
    "# nearest_warehouse column\n",
    "for index,rows in empty_warehouse.iterrows(): \n",
    "    dist_list = []\n",
    "    # Looping through the main dataframe  \n",
    "    for index1, rows1 in df_warehouse.iterrows():\n",
    "        # Calculating distances from each warehouse to the customer \n",
    "        dist_list.append(haversine_fun(rows['customer_lat'],\n",
    "                                       rows['customer_long'],\n",
    "                                       rows1['lat'], rows1['lon']))\n",
    "    main_list.append(dist_list)\n",
    "main_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the index and minimum distance from the \n",
    "# above list for each customer\n",
    "new = []  # list to hold the index of minimum distance \n",
    "min_dist = [] # list to hold the minimum distance\n",
    "for i in main_list:\n",
    "    new.append(i.index(min(i)))\n",
    "    min_dist.append(round(min(i),4))\n",
    "column_name = np.array(new)\n",
    "empty_warehouse['indexo'] = column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the warehouse NaN values \n",
    "min_dist = np.array(min_dist)\n",
    "empty_warehouse['distance_to_nearest_warehouse'] = min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the warehouse names based on the indices of \n",
    "# their corresponding minimum dist\n",
    "empty_warehouse['nearest_warehouse'] = empty_warehouse['indexo']. \\\n",
    "            apply(lambda x: df_warehouse.loc[0,'names'] \n",
    "            if x==0 else \n",
    "            (df_warehouse.loc[1,'names'] if x==1 \n",
    "            else df_warehouse.loc[2,'names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the original dataframe using empty_warehouse dataframe\n",
    "for index, row in empty_warehouse.iterrows(): \n",
    "    for index1, rows1 in df1.iterrows():\n",
    "        if index == index1:\n",
    "            df1.loc[index,'nearest_warehouse'] = row['nearest_warehouse']\n",
    "            df1.loc[index,'distance_to_nearest_warehouse'] = row['distance_to_nearest_warehouse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling missing values in Latest review\n",
    "#### Steps\n",
    "1. Import SentimentIntensityAnalyzer from nltk.sentiment.vader\n",
    "2. Create a SentimentIntensityAnalyzer() object\n",
    "3. For all the reviews in the column \"latest_customer_review\" calculate the compound value:\n",
    "    - if compound is greater than or equal to 0.05 then flag it as '1'\n",
    "    - if compound is less than 0.05 then flag it as '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining if the customer is happy or not using sentimental analysis\n",
    "senti = SentimentIntensityAnalyzer()\n",
    "df1['is_happy_customer'] = df1['latest_customer_review']. \\\n",
    "        apply(lambda x: senti.polarity_scores(x)['compound'] \n",
    "        >= 0.05).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delivery charges missing value\n",
    "> To predict the missing delivery charges we need to train a model based on the seasons since each season has its own calculation for determining the delivery charges. Th following steps are taken in order to train the model and predict the values.\n",
    "\n",
    "### Steps:\n",
    "1. Filter out all the non null values in delivery charges column so that the model we train does not have any missing values in them\n",
    "2. For each of the seasons, filter out the respective seasons from the dataframe into a new dataframe\n",
    "3. Use LinearRegression package from sklearn.linear_model in-order to train the model for each season\n",
    "4. Dependent variable for our model will be delivery_charges since we are supposed to predict the missing values in delivery charges eventually \n",
    "5. Independent variables must be 'distance_to_nearest_warehouse', 'is_expedited_delivery', 'is_happy_customer' since delivery charges depend on these attributes \n",
    "6. Create a model \n",
    "7. Filter out the missing values in column delivery charges in the original dataframe and again filter out the season within it\n",
    "8. Extract the indices of each missing value and use the same in the original dataframe to predict the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering all the non-null values from the original dataframe\n",
    "delivery_df1 = df1[~df1['delivery_charges'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering winter dataset\n",
    "delivery_df1_winter = delivery_df1[delivery_df1['season'] == 'Winter']\n",
    "\n",
    "\n",
    "\n",
    "# Training a model for winter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Target variable is delivery_charges and independent variables \n",
    "# are distance_to_nearest_warehouse,is_expedited_delivery,is_happy_customer\n",
    "y_winter = delivery_df1_winter['delivery_charges']\n",
    "x_winter = delivery_df1_winter[['distance_to_nearest_warehouse', \n",
    "                                'is_expedited_delivery', \n",
    "                                'is_happy_customer']]\n",
    "\n",
    "# A linear fit is found between delivery_charges and \n",
    "# distance_to_nearest_warehouse,is_expedited_delivery,is_happy_customer\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(x_winter,y_winter)\n",
    "\n",
    "# Fetching the missing data of season Winter\n",
    "delivery_df1_missing =  df1[df1['delivery_charges'].isnull()]\n",
    "delivery_df1_missing_winter = delivery_df1_missing[\n",
    "    delivery_df1_missing['season'] == 'Winter']\n",
    "\n",
    "# From the above dataframe extracting the indices\n",
    "missing_values_winter = delivery_df1_missing_winter.index.values\n",
    "\n",
    "\n",
    "# Updating the missing values in delivery charges for Winter \n",
    "# season in the original dataset\n",
    "for i in missing_values_winter:\n",
    "    y_pred_winter = linear_regression.predict(pd.DataFrame(\n",
    "        df1.iloc[i,[13, 12, 15]]).transpose())\n",
    "    df1.iloc[i,6] = round(y_pred_winter[0],2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering summer dataset\n",
    "delivery_df1_summer = delivery_df1[delivery_df1['season'] == 'Summer']\n",
    "\n",
    "\n",
    "# Training a model for summer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Target variable is delivery_charges and independent variables \n",
    "# are distance_to_nearest_warehouse, \n",
    "# is_expedited_delivery, is_happy_customer\n",
    "y_summer = delivery_df1_summer['delivery_charges']\n",
    "x_summer = delivery_df1_summer[['distance_to_nearest_warehouse', \n",
    "                                'is_expedited_delivery', \n",
    "                                'is_happy_customer']]\n",
    "\n",
    "# A linear fit is found between delivery_charges and \n",
    "# distance_to_nearest_warehouse,is_expedited_delivery,is_happy_customer\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(x_summer,y_summer)\n",
    "\n",
    "\n",
    "# Fetching the missing data of season Summer\n",
    "delivery_df1_missing =  df1[df1['delivery_charges'].isnull()]\n",
    "delivery_df1_missing_summer = delivery_df1_missing[\n",
    "    delivery_df1_missing['season'] == 'Summer']\n",
    "\n",
    "# From the above dataframe extracting the indices\n",
    "missing_values_summer = delivery_df1_missing_summer.index.values\n",
    "missing_values_summer\n",
    "\n",
    "\n",
    "# Updating the missing values in delivery charges for Summer \n",
    "# season in the original dataset\n",
    "for i in missing_values_summer:\n",
    "    y_pred_summer = linear_regression.predict(pd.DataFrame(\n",
    "        df1.iloc[i,[13, 12, 15]]).transpose())\n",
    "    df1.iloc[i,6] = round(y_pred_summer[0],2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering spring dataset\n",
    "delivery_df1_spring = delivery_df1[delivery_df1['season'] == 'Spring']\n",
    "\n",
    "\n",
    "# Training a model for spring\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Target variable is delivery_charges and independent variables \n",
    "# are distance_to_nearest_warehouse,is_expedited_delivery,is_happy_customer\n",
    "y_spring = delivery_df1_spring['delivery_charges']\n",
    "x_spring = delivery_df1_spring[['distance_to_nearest_warehouse', \n",
    "                                'is_expedited_delivery',\n",
    "                                'is_happy_customer']]\n",
    "\n",
    "# A linear fit is found between delivery_charges and \n",
    "# distance_to_nearest_warehouse,is_expedited_delivery,is_happy_customer\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(x_spring,y_spring)\n",
    "\n",
    "\n",
    "# Fetching the missing data of season Spring\n",
    "delivery_df1_missing =  df1[df1['delivery_charges'].isnull()]\n",
    "delivery_df1_missing_spring = delivery_df1_missing[\n",
    "    delivery_df1_missing['season'] == 'Spring']\n",
    "\n",
    "# From the above dataframe extracting the indices\n",
    "missing_values_spring = delivery_df1_missing_spring.index.values\n",
    "\n",
    "\n",
    "\n",
    "# Updating the missing values in delivery charges for Spring season in the original dataset\n",
    "for i in missing_values_spring:\n",
    "    y_pred_spring = linear_regression.predict(pd.DataFrame(\n",
    "        df1.iloc[i,[13, 12, 15]]).transpose())\n",
    "    df1.iloc[i,6] = round(y_pred_spring[0],2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering autumn dataset\n",
    "delivery_df1_autumn = delivery_df1[delivery_df1['season'] == 'Autumn']\n",
    "\n",
    "\n",
    "# Training a model for autumn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Target variable is delivery_charges and independent \n",
    "# variables are distance_to_nearest_warehouse,is_expedited_delivery,is_happy_customer\n",
    "y_autumn = delivery_df1_autumn['delivery_charges']\n",
    "x_autumn = delivery_df1_autumn[['distance_to_nearest_warehouse',\n",
    "                                'is_expedited_delivery',\n",
    "                                'is_happy_customer']]\n",
    "\n",
    "# A linear fit is found between delivery_charges and \n",
    "# distance_to_nearest_warehouse,is_expedited_delivery,is_happy_customer\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(x_autumn,y_autumn)\n",
    "\n",
    "\n",
    "\n",
    "# Fetching the missing data of season Autumn\n",
    "delivery_df1_missing =  df1[df1['delivery_charges'].isnull()]\n",
    "delivery_df1_missing_autumn = delivery_df1_missing[\n",
    "    delivery_df1_missing['season'] == 'Autumn']\n",
    "\n",
    "# From the above dataframe extracting the indices\n",
    "missing_values_autumn = delivery_df1_missing_autumn.index.values\n",
    "\n",
    "\n",
    "\n",
    "# Updating the missing values in delivery charges for Autumn \n",
    "# season in the original dataset\n",
    "for i in missing_values_autumn:\n",
    "    y_pred_autumn = linear_regression.predict(pd.DataFrame(\n",
    "        df1.iloc[i,[13, 12, 15]]).transpose())\n",
    "    df1.iloc[i,6] = round(y_pred_autumn[0],2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling missing values in order_price and order_total\n",
    "#### Steps:\n",
    "In this method, the item name in shopping_cart will act like 'variables' and no. of items will act as the 'coefficients'. And the entire list of items shopped by a customer will act like an equation which will be used for linear algebra computation \n",
    "\n",
    "1. Creating a new column (df1['shopping_cart_dict']) containing the shopping cart values but in dictionary format\n",
    "\n",
    "2. Sorting the column df1['shopping_cart_dict'] inorder to get similar shopping_cart values together\n",
    "\n",
    "3. Unzipping the shopping cart tuple and creating two new columns:-\n",
    "    - crazy_df['coeff'] --> which will have only the coefficients of the equation(shopping cart)\n",
    "    - crazy_df['variables'] --> which will have only the variables(items) of the equation(shopping cart)\n",
    "4. Create a new dataframe which will have all the items of a shopping cart and their respective number of occurances \n",
    "5. Merge this dataframe with crazy_df to get the number of equations present and have it in a column called 'value'\n",
    "6. Create another column named ('variable_count') to hold the number of variables in an equation\n",
    "7. Drop any row which has a null value in the column 'delivery_charges' (since it cannot be used to calculate the variable value using linalg function)\n",
    "8. Filter out this dataframe into chunks of dataframes which are square in nature (in other words keep only those dataframes whose number of equations and number of variables are the same!) and append it to a list.\n",
    "    - This list will have multiple square dataframes as each element \n",
    "9. Calculate the price of each product using linalg from numpy and store it in a dictionary:\n",
    "    - This dictionary will have prices for each product (no. of products is 10)\n",
    "10. Using this dictionary and the items in the shopping cart, the order price can be calculated. \n",
    "11. Using the calculated order_price, the order_total can be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting shopping cart values to dictionary and sorting it\n",
    "df1['shopping_cart_dict'] = df1['shopping_cart'].apply(\n",
    "    lambda x: dict(eval(x))) \n",
    "\n",
    "# Sorting the items based on the keys of the dictionary\n",
    "df1['shopping_cart_dict'] = df1['shopping_cart_dict'].apply(\n",
    "    lambda x: list(sorted(x.items())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dataframe based on the new column just created\n",
    "crazy_df = df1.sort_values('shopping_cart_dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the coeff, variable names \n",
    "\n",
    "# Using the unzip functionality to separate out the items \n",
    "# and their corresponding item numbers in two different columns\n",
    "# (coefficient and variables)\n",
    "crazy_df['coeff'] = crazy_df['shopping_cart_dict'].apply(\n",
    "    lambda x: list(list(zip(*x))[1]))\n",
    "crazy_df['variables'] = crazy_df['shopping_cart_dict'].apply(\n",
    "    lambda x: list(zip(*x))[0])\n",
    "\n",
    "# Create a new dataframe which will hold the count of the variables\n",
    "variable_count_df = pd.DataFrame(columns=['items']) \n",
    "\n",
    "# 'items' column will have item names and 'value' \n",
    "# will have how many times the 'items' have repeated\n",
    "variable_count_df['items'] = crazy_df['variables'].value_counts().index\n",
    "variable_count_df['value'] = crazy_df['variables'].value_counts().values\n",
    "variable_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining both the data frames\n",
    "value_count_df = pd.merge(crazy_df, variable_count_df, \n",
    "                          left_on='variables',right_on='items',how='inner')\n",
    "\n",
    "# Calculating the variable count and retaining only square matrix\n",
    "# Create a new column to hold the number of variables in each equation \n",
    "value_count_df['variable_count'] = value_count_df['variables'].apply(\n",
    "    lambda x: len(x))\n",
    "value_count_df = value_count_df.dropna(how='any', axis = 0)\n",
    "\n",
    "# dataframes which has square matrix charateristics\n",
    "square_df = value_count_df[value_count_df['value'] == \n",
    "                           value_count_df['variable_count']]\n",
    "\n",
    "# List of dataframes which has square matrix charateristics\n",
    "list_of_df = []\n",
    "for i in np.unique(square_df['variables'].values):\n",
    "    list_of_df.append(square_df[square_df['variables'] == i])\n",
    "list_of_df[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating price of each product using linalg from numpy\n",
    "\n",
    "# Empty dictionary to hold the product names and their respective price\n",
    "product_list = {} \n",
    "\n",
    "# Looping through the list of dataframes\n",
    "for i in list_of_df:  \n",
    "    \n",
    "    # Check if number of variables of an equation is equal \n",
    "    # to the number of equation (square matrix) in order \n",
    "    # to be used in simultaneous equation\n",
    "    if i['variable_count'].value_counts().index.values == \\\n",
    "    i['variable_count'].value_counts().values:\n",
    "        \n",
    "        # Try except block is used to avoid the \"Singular matrix\"\n",
    "        # error thrown by linalg.solve() function \n",
    "        # which occurs when there are many solutions\n",
    "        try: \n",
    "            \n",
    "            # Take the list of coefficients\n",
    "            a = np.array(list(i['coeff'].values))  \n",
    "            \n",
    "            # Take the list of order price \n",
    "            b = np.array(list(i['order_price'].values)) \n",
    "            \n",
    "            # Solve for all the variables in the equation\n",
    "            x = np.linalg.solve(a, b) \n",
    "            \n",
    "            # If the solution is correct and non-negative condition \n",
    "            if np.allclose(np.dot(a, x), b) and x[1] > 0:  \n",
    "                # is checked, then update the product_list\n",
    "                product_list.update(dict(zip(i['items'].values[0],x)))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the price of each product\n",
    "product_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the order price column in the original dataframe\n",
    "\n",
    "# List of order price at the end of running this code cell\n",
    "cal_order_price = []  \n",
    "\n",
    "# Looping through each shopping_cart value\n",
    "for i in df1['shopping_cart'].values: \n",
    "    sum_val = 0 \n",
    "    \n",
    "    # Looping withing the shopping_cart items\n",
    "    for j in eval(i): \n",
    "        \n",
    "        # price of item is taken from the product_list dictionary \n",
    "        # and multiplied by the no. of items\n",
    "        sum_val += product_list[j[0]] * j[1] \n",
    "                                             \n",
    "        \n",
    "    cal_order_price.append(sum_val)  \n",
    "# Updating the entire column of 'order_price'\n",
    "df1['order_price'] = cal_order_price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the missing values in order total column in original dataframe\n",
    "df1['order_total'] = df1['order_price'] * \\\n",
    "(1 - (df1['coupon_discount']/100)) + df1['delivery_charges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.iloc[:,1:16]\n",
    "df1.to_csv('31009751_missing_data_output.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data in as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('31009751_outlier_data.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a boxplot to check the outliers in the column 'delivery_charge'\n",
    "There are many outliers in the the column delivery_charges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.boxplot(column=['delivery_charges'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model to predict the delivery_charges\n",
    "1. delivery_charges depends on 'is_expedited_delivery', 'distance_to_nearest_warehouse' and 'is_happy_customer' attributes\n",
    "2. Predict the values\n",
    "3. After predicting the values subtract it from the original value\n",
    "4. Calculate the z score of the column values \n",
    "5. Consider the lower and upper to be 2 sigma and anything below or above this will be an outlier\n",
    "6. Filter out the data for which the zscore column is 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a linear model on the column delivery_charges \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Target variable is delivery_charges and \n",
    "# independent variables are distance_to_nearest_warehouse, \n",
    "# is_expedited_delivery, is_happy_customer\n",
    "y_var = df2['delivery_charges']\n",
    "x_var = df2[['is_expedited_delivery',\n",
    "             'distance_to_nearest_warehouse', \n",
    "             'is_happy_customer']]\n",
    "\n",
    "# A linear fit is found between delivery_charges and \n",
    "# distance_to_nearest_warehouse,is_expedited_delivery,is_happy_customer\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(x_var, y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the delivery_charges for all the rows\n",
    "df2['predicted_delivery_charges'] = linear_regression.predict(\n",
    "    pd.DataFrame(df2.iloc[:,[12, 13, 15]]))\n",
    "\n",
    "# Calculating the difference between delivery_charges \n",
    "# and predicted delivery charges and taking its absolute value\n",
    "df2['delta/residual'] = abs(\n",
    "    df2['delivery_charges'] - df2['predicted_delivery_charges'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the z score\n",
    "df2['zScore'] = df2['delta/residual'].apply(\n",
    "    lambda x: (x - df2['delta/residual'].mean())/ \\\n",
    "    df2['delta/residual'].std(ddof=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for z score more than 2 sigma and convert any \n",
    "# outlier to 1 else use 0. \n",
    "df2['outlier'] = (abs(df2['zScore']) > 2).astype(int).values\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the data for which the outlier column value is 0\n",
    "df_without_outlier = df2[df2['outlier']==0] \n",
    "\n",
    "# Select only the first 15 columns\n",
    "df_without_outlier = df_without_outlier.iloc[:,0:16] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writting the data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_outlier.to_csv('31009751_outlier_data_output.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_outlier.boxplot(column=['delivery_charges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "Haversine function: https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
